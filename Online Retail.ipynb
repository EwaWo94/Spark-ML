{"cells":[{"source":"![Shopping trolley in front of a laptop](./iStock-1249219777.jpg)\n\nIt's simple to buy any product with a click and have it delivered to your door. Online shopping has been rapidly evolving over the last few years, making our lives easier. But behind the scenes, e-commerce companies face a complex challenge that needs to be addressed. \n\nUncertainty plays a big role in how the supply chains plan and organize their operations to ensure that the products are delivered on time. These uncertainties can lead to challenges such as stockouts, delayed deliveries, and increased operational costs.\n\nYou work for the Sales & Operations Planning (S&OP) team at a multinational e-commerce company. They need your help to assist in planning for the upcoming end-of-the-year sales. They want to use your insights to plan for promotional opportunities and manage their inventory. This effort is to ensure they have the right products in stock when needed and ensure their customers are satisfied with the prompt delivery to their doorstep.\n\n\n## The Data\n\nYou are provided with a sales dataset to use. A summary and preview are provided below.\n\n# Online Retail.csv\n\n| Column     | Description              |\n|------------|--------------------------|\n| `'InvoiceNo'` | A 6-digit number uniquely assigned to each transaction |\n| `'StockCode'` | A 5-digit number uniquely assigned to each distinct product |\n| `'Description'` | The product name |\n| `'Quantity'` | The quantity of each product (item) per transaction |\n| `'UnitPrice'` | Product price per unit |\n| `'CustomerID'` | A 5-digit number uniquely assigned to each customer |\n| `'Country'` | The name of the country where each customer resides |\n| `'InvoiceDate'` | The day and time when each transaction was generated `\"MM/DD/YYYY\"` |\n| `'Year'` | The year when each transaction was generated |\n| `'Month'` | The month when each transaction was generated |\n| `'Week'` | The week when each transaction was generated (`1`-`52`) |\n| `'Day'` | The day of the month when each transaction was generated (`1`-`31`) |\n| `'DayOfWeek'` | The day of the weeke when each transaction was generated <br>(`0` = Monday, `6` = Sunday) |","metadata":{},"id":"6918e18a-c248-4929-b552-7aee2057c0eb","cell_type":"markdown"},{"source":"# Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek, lit\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Initialize Spark session\nmy_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n\n# Importing sales data\nsales_data = my_spark.read.csv(\n    \"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n\n# Convert InvoiceDate to datetime \nsales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))","metadata":{"executionCancelledAt":null,"executionTime":813,"lastExecutedAt":1744717558696,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek, lit\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Initialize Spark session\nmy_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n\n# Importing sales data\nsales_data = my_spark.read.csv(\n    \"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n\n# Convert InvoiceDate to datetime \nsales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":38,"type":"stream"}},"lastExecutedByKernel":"a177e71c-7d81-40ea-9b98-e67272f81f01"},"id":"81a07c66-a3d4-4fdd-9c3c-7b3a19b80d62","cell_type":"code","execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":"                                                                                \r"}]},{"source":"# STEP 1: Group the data into daily intervals, using the .groupBy() method on the following columns: \"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\", saving as a new variable. Use .agg() on the grouped data, which accepts a dictionary with column names as keys and the type of calculation as values. With this approach, get the \"sum\" of the \"Quantity\" column and the \"avg\" of \"UnitPrice\".\nsales_data_grouped = sales_data.groupBy(\"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\").agg({\"Quantity\": \"sum\", \"UnitPrice\": \"avg\"})\n\n# To keep the target name consistent, rename the aggregated \"sum(Quantity)\" column to \"Quantity\" using the .withColumnRenamed() method.\nsales_data_grouped = sales_data_grouped.withColumnRenamed('Quantity', \"sum(Quantity)\").show()","metadata":{"executionCancelledAt":null,"executionTime":977,"lastExecutedAt":1744717559673,"lastExecutedByKernel":"a177e71c-7d81-40ea-9b98-e67272f81f01","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# STEP 1: Group the data into daily intervals, using the .groupBy() method on the following columns: \"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\", saving as a new variable. Use .agg() on the grouped data, which accepts a dictionary with column names as keys and the type of calculation as values. With this approach, get the \"sum\" of the \"Quantity\" column and the \"avg\" of \"UnitPrice\".\nsales_data_grouped = sales_data.groupBy(\"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\").agg({\"Quantity\": \"sum\", \"UnitPrice\": \"avg\"})\n\n# To keep the target name consistent, rename the aggregated \"sum(Quantity)\" column to \"Quantity\" using the .withColumnRenamed() method.\nsales_data_grouped = sales_data_grouped.withColumnRenamed('Quantity', \"sum(Quantity)\").show()","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":563,"type":"stream"},"2":{"height":38,"type":"stream"}}},"cell_type":"code","id":"15cfb966-c961-400f-9503-dbcadf388c97","outputs":[{"output_type":"stream","name":"stderr","text":"[Stage 77:>                                                         (0 + 2) / 2]\r"},{"output_type":"stream","name":"stdout","text":"+--------------+---------+-----------+----+-----+---+----+---------+--------------+-------------+\n|       Country|StockCode|InvoiceDate|Year|Month|Day|Week|DayOfWeek|avg(UnitPrice)|sum(Quantity)|\n+--------------+---------+-----------+----+-----+---+----+---------+--------------+-------------+\n|United Kingdom|    22912| 2010-01-12|2010|    1| 12|   2|        1|          4.95|            3|\n|        France|    22659| 2010-01-12|2010|    1| 12|   2|        1|          1.95|           24|\n|United Kingdom|    21544| 2010-01-12|2010|    1| 12|   2|        1|          0.85|           12|\n|United Kingdom|    21098| 2010-01-12|2010|    1| 12|   2|        1|          1.25|           16|\n|        Norway|    85150| 2010-01-12|2010|    1| 12|   2|        1|          2.55|           12|\n|United Kingdom|    22195| 2010-01-12|2010|    1| 12|   2|        1|          1.65|            5|\n|United Kingdom|    21668| 2010-01-12|2010|    1| 12|   2|        1|          1.25|           18|\n|United Kingdom|    21844| 2010-02-12|2010|    2| 12|   6|        4|          2.95|           12|\n|United Kingdom|    22914| 2010-02-12|2010|    2| 12|   6|        4|          4.95|            3|\n|United Kingdom|    21990| 2010-03-12|2010|    3| 12|  10|        4|          2.95|            6|\n|United Kingdom|   84032A| 2010-03-12|2010|    3| 12|  10|        4|          2.95|           18|\n|United Kingdom|    22670| 2010-03-12|2010|    3| 12|  10|        4|          1.25|            1|\n|        France|    21559| 2010-05-12|2010|    5| 12|  19|        2|          2.55|            6|\n|United Kingdom|    21980| 2010-05-12|2010|    5| 12|  19|        2|          0.29|           24|\n|United Kingdom|    22034| 2010-05-12|2010|    5| 12|  19|        2|          0.42|            3|\n|United Kingdom|    84987| 2010-05-12|2010|    5| 12|  19|        2|          1.45|            1|\n|       Germany|    84920| 2010-05-12|2010|    5| 12|  19|        2|          3.75|            4|\n|United Kingdom|    22832| 2010-05-12|2010|    5| 12|  19|        2|         10.75|            2|\n|United Kingdom|    22315| 2010-06-12|2010|    6| 12|  23|        5|          1.25|           14|\n|United Kingdom|    21754| 2010-06-12|2010|    6| 12|  23|        5|         5.825|           30|\n+--------------+---------+-----------+----+-----+---+----+---------+--------------+-------------+\nonly showing top 20 rows\n\n"},{"output_type":"stream","name":"stderr","text":"                                                                                \r"}],"execution_count":39},{"source":"","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1744717559726,"lastExecutedByKernel":"a177e71c-7d81-40ea-9b98-e67272f81f01","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":""},"cell_type":"code","id":"00d9a8c9-e409-4b62-b4d9-88042e779aa5","outputs":[],"execution_count":39},{"source":"# STEP 2: Splitting data into 2 datasets\n# Sales data till 2011-09-25 (incl.)\ntrain_data = sales_data.filter(col(\"InvoiceDate\") <= lit(\"2011-09-25\"))\n# Sales data after 2011-09-25\ntest_data = sales_data.filter(col(\"InvoiceDate\") > lit(\"2011-09-25\"))\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1744718233630,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# STEP 2: Splitting data into 2 datasets\n# Sales data till 2011-09-25 (incl.)\ntrain_data = sales_data.filter(col(\"InvoiceDate\") <= lit(\"2011-09-25\"))\n# Sales data after 2011-09-25\ntest_data = sales_data.filter(col(\"InvoiceDate\") > lit(\"2011-09-25\"))\n","lastExecutedByKernel":"a177e71c-7d81-40ea-9b98-e67272f81f01","outputsMetadata":{"0":{"height":501,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"0d1fae10-2f4b-4871-86c4-eccdeec6036c","nodeType":"const"}}},"1":{"height":50,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"0d1fae10-2f4b-4871-86c4-eccdeec6036c","nodeType":"const"}}}}},"id":"b5106e04-f9da-459f-a1cc-14e437fe001d","cell_type":"code","execution_count":45,"outputs":[]},{"source":"# STEP 3: building regression model\n# Use StringIndexer() to encode your categorical columns ('Country' and 'StockCode'), saving as country_indexer and stock_code_indexer. When doing this, chain the String Indexer's .setHandleInvalid() method, setting to \"keep\"\n\ncountry_indexer = StringIndexer(inputCol=\"Country\", outputCol=\"CountryIndex\").setHandleInvalid(\"keep\")\nstock_code_indexer = StringIndexer(inputCol=\"StockCode\", outputCol=\"StockCodeIndex\").setHandleInvalid(\"keep\")\n\n# combine all selected features into a vector, providing a list of features to the inputCols argument and setting the outputCol to a name for the features\n\nassembler = VectorAssembler(inputCols=[\"CountryIndex\", \"StockCodeIndex\"], outputCol=\"features\")\n\n# RandomForestModel\n\nrf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Quantity\", maxBins=4000)\n\n# Pipeline - combining all steps\n\npipeline = Pipeline(stages= [country_indexer, stock_code_indexer, assembler, rf])\n\n# model\n\nmodel = pipeline.fit(train_data)","metadata":{"executionCancelledAt":null,"executionTime":12276,"lastExecutedAt":1744717574668,"lastExecutedByKernel":"a177e71c-7d81-40ea-9b98-e67272f81f01","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# STEP 3: building regression model\n# Use StringIndexer() to encode your categorical columns ('Country' and 'StockCode'), saving as country_indexer and stock_code_indexer. When doing this, chain the String Indexer's .setHandleInvalid() method, setting to \"keep\"\n\ncountry_indexer = StringIndexer(inputCol=\"Country\", outputCol=\"CountryIndex\").setHandleInvalid(\"keep\")\nstock_code_indexer = StringIndexer(inputCol=\"StockCode\", outputCol=\"StockCodeIndex\").setHandleInvalid(\"keep\")\n\n# combine all selected features into a vector, providing a list of features to the inputCols argument and setting the outputCol to a name for the features\n\nassembler = VectorAssembler(inputCols=[\"CountryIndex\", \"StockCodeIndex\"], outputCol=\"features\")\n\n# RandomForestModel\n\nrf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Quantity\", maxBins=4000)\n\n# Pipeline - combining all steps\n\npipeline = Pipeline(stages= [country_indexer, stock_code_indexer, assembler, rf])\n\n# model\n\nmodel = pipeline.fit(train_data)","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":38,"type":"stream"},"3":{"height":38,"type":"stream"},"4":{"height":38,"type":"stream"},"5":{"height":38,"type":"stream"},"6":{"height":38,"type":"stream"}}},"cell_type":"code","id":"65da9c32-2aec-48fa-abfe-5b6c1ffd21fc","outputs":[{"output_type":"stream","name":"stderr","text":"                                                                                \r"},{"output_type":"stream","name":"stdout","text":"25/04/15 11:46:07 WARN DAGScheduler: Broadcasting large task binary with size 1106.4 KiB\n"},{"output_type":"stream","name":"stderr","text":"                                                                                \r"},{"output_type":"stream","name":"stdout","text":"25/04/15 11:46:08 WARN DAGScheduler: Broadcasting large task binary with size 1855.8 KiB\n"},{"output_type":"stream","name":"stderr","text":"                                                                                \r"},{"output_type":"stream","name":"stdout","text":"25/04/15 11:46:11 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n"},{"output_type":"stream","name":"stderr","text":"                                                                                \r"}],"execution_count":41},{"source":"# STEP 4 Evaluating the model (MAE)\n\ntest_predictions = model.transform(test_data)\n\ntest_predictions = test_predictions.withColumn(\"prediction\", col(\"prediction\").cast(\"double\"))\n\n# Calculate MAE\n\nmae_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol='Quantity', metricName='mae')\nmae = mae_evaluator.evaluate(test_predictions)\nprint(mae)","metadata":{"executionCancelledAt":null,"executionTime":1666,"lastExecutedAt":1744717576334,"lastExecutedByKernel":"a177e71c-7d81-40ea-9b98-e67272f81f01","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# STEP 4 Evaluating the model (MAE)\n\ntest_predictions = model.transform(test_data)\n\ntest_predictions = test_predictions.withColumn(\"prediction\", col(\"prediction\").cast(\"double\"))\n\n# Calculate MAE\n\nmae_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol='Quantity', metricName='mae')\nmae = mae_evaluator.evaluate(test_predictions)\nprint(mae)","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":38,"type":"stream"}}},"cell_type":"code","id":"508cde75-cd3c-477c-9446-4ba580b4f2de","outputs":[{"output_type":"stream","name":"stderr","text":"[Stage 99:=============================>                            (1 + 1) / 2]\r"},{"output_type":"stream","name":"stdout","text":"5.410738521647606\n"},{"output_type":"stream","name":"stderr","text":"                                                                                \r"}],"execution_count":42},{"source":"# STEP 5: Identify the quantity sold at specific week\nquantity_sold_w39 = int(test_predictions.groupBy(['Year', 'Week']).agg({'prediction': 'sum'}).filter((col('Week')==39) & (col('Year')==2011)).select('sum(prediction)').collect()[0][0])\nprint(quantity_sold_w39)","metadata":{"executionCancelledAt":null,"executionTime":676,"lastExecutedAt":1744717577010,"lastExecutedByKernel":"a177e71c-7d81-40ea-9b98-e67272f81f01","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# STEP 5: Identify the quantity sold at specific week\nquantity_sold_w39 = int(test_predictions.groupBy(['Year', 'Week']).agg({'prediction': 'sum'}).filter((col('Week')==39) & (col('Year')==2011)).select('sum(prediction)').collect()[0][0])\nprint(quantity_sold_w39)","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":38,"type":"stream"}}},"cell_type":"code","id":"09eb74e1-ed91-4aed-ad04-a672e6119d68","outputs":[{"output_type":"stream","name":"stdout","text":"87346\n"}],"execution_count":43}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}